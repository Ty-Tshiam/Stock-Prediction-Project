{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Project \n",
    "Objective: Predict future stock prices (e.g., next day, week, or month) using historical data and external factors.\n",
    "\n",
    "Type of Prediction:\n",
    "\n",
    "Regression: Predict exact future prices.\n",
    "\n",
    "Classification: Predict price movement direction (up/down/stable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Open      High       Low     Close  Volume  \\\n",
      "Date                                                                        \n",
      "1962-01-02 00:00:00-05:00  0.623745  0.634141  0.617508  0.621666  432682   \n",
      "1962-01-03 00:00:00-05:00  0.618548  0.618548  0.613350  0.615429  296467   \n",
      "1962-01-04 00:00:00-05:00  0.615429  0.620627  0.602954  0.608152  368581   \n",
      "1962-01-05 00:00:00-05:00  0.608152  0.609192  0.582163  0.592559  546862   \n",
      "1962-01-08 00:00:00-05:00  0.592558  0.592558  0.573846  0.591519  620978   \n",
      "\n",
      "                           Dividends  Stock Splits  \n",
      "Date                                                \n",
      "1962-01-02 00:00:00-05:00        0.0           0.0  \n",
      "1962-01-03 00:00:00-05:00        0.0           0.0  \n",
      "1962-01-04 00:00:00-05:00        0.0           0.0  \n",
      "1962-01-05 00:00:00-05:00        0.0           0.0  \n",
      "1962-01-08 00:00:00-05:00        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "ticker = \"GE\"\n",
    "\n",
    "df = yf.Ticker(ticker).history(period=\"max\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Using Data Wrangler the data seems pretty clean but to ensure data integrity, I'm going to use some data cleaning methods using pandas.\n",
    "\n",
    "We have to ensure there's no missing data, a consistent format, removal of unwanted observations, incorrect data, and removal of duplicates. We don't need to manage outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removal of rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensures that the dates are in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "\n",
    "if df[\"Date\"].dt.tz is not None:\n",
    "    df[\"Date\"] = df[\"Date\"].dt.tz_convert(None)\n",
    "else:\n",
    "    df[\"Date\"] = df[\"Date\"].dt.tz_localize(\"UTC\")  \n",
    "    df[\"Date\"] = df[\"Date\"].dt.tz_convert(None)   \n",
    "df[\"Date\"] = df[\"Date\"].dt.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensures that there is no wrong data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove('Date')\n",
    "columns.remove('Volume')\n",
    "\n",
    "# Ensure that the columns are of the correct data type\n",
    "for column in columns:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce', downcast='float')\n",
    "\n",
    "df['Volume'] = pd.to_numeric(df['Volume'], errors='coerce', downcast='integer')\n",
    "\n",
    "\n",
    "#Make sure that all numbers are positive, if not make it positive\n",
    "columns.append('Volume')\n",
    "for column in columns:\n",
    "    for i in df.index:\n",
    "        if df.loc[i, column] < 0:\n",
    "            df.loc[i, column] = df.loc[i, column] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data \n",
    "\n",
    "* This is where I will be collecting more data for the AI to learn from as well as do some feature engineering. A lot of repeated steps for previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date        Open        High         Low       Close   Volume  \\\n",
      "15883 2025-02-10  206.139999  206.660004  203.399994  205.220001  3787500   \n",
      "15884 2025-02-11  205.000000  209.149994  204.440002  208.820007  4110900   \n",
      "15885 2025-02-12  206.699997  211.419998  206.139999  209.639999  4595200   \n",
      "15886 2025-02-13  211.119995  211.300003  206.270004  208.360001  3763300   \n",
      "15887 2025-02-14  208.660004  209.160004  204.970001  208.270004  4267200   \n",
      "\n",
      "       Dividends  Stock Splits      SMA_10      SMA_50     SMA_100     SMA_200  \n",
      "15883        0.0           0.0  203.010001  180.747072  181.801186  172.836767  \n",
      "15884        0.0           0.0  204.449002  181.237477  182.091682  173.088728  \n",
      "15885        0.0           0.0  205.503001  181.824152  182.358277  173.334488  \n",
      "15886        0.0           0.0  205.782001  182.354077  182.586053  173.568424  \n",
      "15887        0.0           0.0  206.252000  182.916547  182.800270  173.791262  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['SMA_10'] = df['Close'].rolling(window = 10).mean()\n",
    "df['SMA_50'] = df['Close'].rolling(window = 50).mean()\n",
    "df['SMA_100'] = df['Close'].rolling(window = 100).mean()\n",
    "df['SMA_200'] = df['Close'].rolling(window = 200).mean()\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date        Open        High         Low       Close   Volume  \\\n",
      "15883 2025-02-10  206.139999  206.660004  203.399994  205.220001  3787500   \n",
      "15884 2025-02-11  205.000000  209.149994  204.440002  208.820007  4110900   \n",
      "15885 2025-02-12  206.699997  211.419998  206.139999  209.639999  4595200   \n",
      "15886 2025-02-13  211.119995  211.300003  206.270004  208.360001  3763300   \n",
      "15887 2025-02-14  208.660004  209.160004  204.970001  208.270004  4267200   \n",
      "\n",
      "       Dividends  Stock Splits      SMA_10      SMA_50     SMA_100  \\\n",
      "15883        0.0           0.0  203.010001  180.747072  181.801186   \n",
      "15884        0.0           0.0  204.449002  181.237477  182.091682   \n",
      "15885        0.0           0.0  205.503001  181.824152  182.358277   \n",
      "15886        0.0           0.0  205.782001  182.354077  182.586053   \n",
      "15887        0.0           0.0  206.252000  182.916547  182.800270   \n",
      "\n",
      "          SMA_200      EMA_10      EMA_50     EMA_100    EMA_200  \n",
      "15883  172.836767  201.969537  186.177389  180.182502  92.082631  \n",
      "15884  173.088728  203.215077  187.065334  180.749582  92.199310  \n",
      "15885  173.334488  204.383245  187.950615  181.321669  92.316692  \n",
      "15886  173.568424  205.106291  188.750983  181.857082  92.432678  \n",
      "15887  173.791262  205.681512  189.516435  182.380110  92.548457  \n"
     ]
    }
   ],
   "source": [
    "df['EMA_10'] = df['Close'].ewm(span=10).mean()\n",
    "df['EMA_50'] = df['Close'].ewm(span=50).mean()\n",
    "df['EMA_100'] = df['Close'].ewm(span=100).mean()\n",
    "df['EMA_200'] = df['Close'].ewm(span=2000).mean()\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date        Open        High         Low       Close   Volume  \\\n",
      "15883 2025-02-10  206.139999  206.660004  203.399994  205.220001  3787500   \n",
      "15884 2025-02-11  205.000000  209.149994  204.440002  208.820007  4110900   \n",
      "15885 2025-02-12  206.699997  211.419998  206.139999  209.639999  4595200   \n",
      "15886 2025-02-13  211.119995  211.300003  206.270004  208.360001  3763300   \n",
      "15887 2025-02-14  208.660004  209.160004  204.970001  208.270004  4267200   \n",
      "\n",
      "       Dividends  Stock Splits      SMA_10      SMA_50  ...  Bollinger_High  \\\n",
      "15883        0.0           0.0  203.010001  180.747072  ...      216.224343   \n",
      "15884        0.0           0.0  204.449002  181.237477  ...      216.226974   \n",
      "15885        0.0           0.0  205.503001  181.824152  ...      216.538182   \n",
      "15886        0.0           0.0  205.782001  182.354077  ...      216.579162   \n",
      "15887        0.0           0.0  206.252000  182.916547  ...      215.856579   \n",
      "\n",
      "       Bollinger_Low       ATR         OBV   Chaikin        ADX    SAR_Down  \\\n",
      "15883     172.703660  4.529805  2476039671  0.127563  38.797602  173.699997   \n",
      "15884     176.426029  4.542676  2480150571  0.136994  39.326695  173.699997   \n",
      "15885     179.392821  4.595341  2484745771  0.118149  40.084654  173.699997   \n",
      "15886     182.158842  4.626388  2480982471  0.090183  40.788473  173.699997   \n",
      "15887     185.734424  4.595218  2476715271  0.138776  40.970128  211.419998   \n",
      "\n",
      "           SAR_Up  MA_Crossover   BB_Width  \n",
      "15883  200.738861     22.262928  43.520683  \n",
      "15884  201.733017     23.211525  39.800946  \n",
      "15885  202.919739     23.678849  37.145360  \n",
      "15886  204.440002     23.427923  34.420320  \n",
      "15887  204.440002     23.335453  30.122155  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "import ta\n",
    "\n",
    "df['RSI'] = ta.momentum.rsi(df['Close'], window = 14, fillna = True)\n",
    "df['MACD'] = ta.trend.macd(df['Close'], fillna = True)\n",
    "df['MACD_signal'] = ta.trend.macd_signal(df['Close'], fillna = True)\n",
    "df['Stochastic'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['Bollinger_High'] = ta.volatility.bollinger_hband(df['Close'], fillna = True)\n",
    "df['Bollinger_Low'] = ta.volatility.bollinger_lband(df['Close'], fillna = True)\n",
    "df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'], fillna = True)\n",
    "df['Chaikin'] = ta.volume.chaikin_money_flow(df['High'], df['Low'], df['Close'], df['Volume'], fillna = True)\n",
    "df['ADX'] = ta.trend.adx(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['SAR_Down'] = ta.trend.psar_down(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['SAR_Up'] = ta.trend.psar_up(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['MA_Crossover'] = df['SMA_10'] - df['SMA_50']\n",
    "df['BB_Width'] = df['Bollinger_High'] - df['Bollinger_Low']\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp  value\n",
      "0      2025-02-13   4.33\n",
      "1      2025-02-12   4.33\n",
      "2      2025-02-11   4.33\n",
      "3      2025-02-10   4.33\n",
      "4      2025-02-09   4.33\n",
      "...           ...    ...\n",
      "25791  1954-07-05   0.88\n",
      "25792  1954-07-04   1.25\n",
      "25793  1954-07-03   1.25\n",
      "25794  1954-07-02   1.25\n",
      "25795  1954-07-01   1.13\n",
      "\n",
      "[25796 rows x 2 columns]\n",
      "       timestamp    value\n",
      "0     2025-01-01  317.671\n",
      "1     2024-12-01  315.605\n",
      "2     2024-11-01  315.493\n",
      "3     2024-10-01  315.664\n",
      "4     2024-09-01  315.301\n",
      "...          ...      ...\n",
      "1340  1913-05-01    9.700\n",
      "1341  1913-04-01    9.800\n",
      "1342  1913-03-01    9.800\n",
      "1343  1913-02-01    9.800\n",
      "1344  1913-01-01    9.800\n",
      "\n",
      "[1345 rows x 2 columns]\n",
      "      timestamp  value\n",
      "0    2025-01-01    4.0\n",
      "1    2024-12-01    4.1\n",
      "2    2024-11-01    4.2\n",
      "3    2024-10-01    4.1\n",
      "4    2024-09-01    4.1\n",
      "..          ...    ...\n",
      "920  1948-05-01    3.5\n",
      "921  1948-04-01    3.9\n",
      "922  1948-03-01    4.0\n",
      "923  1948-02-01    3.8\n",
      "924  1948-01-01    3.4\n",
      "\n",
      "[925 rows x 2 columns]\n",
      "   fiscalDateEnding reportedEPS\n",
      "0        2024-12-31        4.49\n",
      "1        2023-12-31         2.8\n",
      "2        2022-12-31        3.01\n",
      "3        2021-12-31        2.13\n",
      "4        2020-12-31        0.32\n",
      "5        2019-12-31        4.88\n",
      "6        2018-12-31        5.28\n",
      "7        2017-12-31         8.4\n",
      "8        2016-12-31          12\n",
      "9        2015-12-31       11.44\n",
      "10       2014-12-31       13.28\n",
      "11       2013-12-31        12.8\n",
      "12       2012-12-31       12.16\n",
      "13       2011-12-31       10.96\n",
      "14       2010-12-31        9.28\n",
      "15       2009-12-31        8.16\n",
      "16       2008-12-31       14.32\n",
      "17       2007-12-31        17.2\n",
      "18       2006-12-31       15.92\n",
      "19       2005-12-31       14.48\n",
      "20       2004-12-31       12.72\n",
      "21       2003-12-31        12.4\n",
      "22       2002-12-31       12.08\n",
      "23       2001-12-31       11.28\n",
      "24       2000-12-31       10.24\n",
      "25       1999-12-31        8.64\n",
      "26       1998-12-31        0.94\n",
      "27       1997-12-31        0.84\n",
      "28       1996-12-31        0.73\n",
      "    fiscalDateEnding reportedDate reportedEPS estimatedEPS surprise  \\\n",
      "0         2024-12-31   2025-01-21        1.32         1.05     0.27   \n",
      "1         2024-09-30   2024-10-22        1.15         1.14     0.01   \n",
      "2         2024-06-30   2024-07-23         1.2         0.99     0.21   \n",
      "3         2024-03-31   2024-04-23        0.82         0.66     0.16   \n",
      "4         2023-12-31   2024-01-23        1.03         0.91     0.12   \n",
      "..               ...          ...         ...          ...      ...   \n",
      "111       1997-03-31   1997-04-10        0.17         0.17        0   \n",
      "112       1996-12-31   1997-01-16        0.21         0.21        0   \n",
      "113       1996-09-30   1996-10-10        0.18         0.18        0   \n",
      "114       1996-06-30   1996-07-17        0.19         0.19        0   \n",
      "115       1996-03-31   1996-04-18        0.15         0.15        0   \n",
      "\n",
      "    surprisePercentage  reportTime  \n",
      "0              25.7143  pre-market  \n",
      "1               0.8772  pre-market  \n",
      "2              21.2121  pre-market  \n",
      "3              24.2424  pre-market  \n",
      "4              13.1868  pre-market  \n",
      "..                 ...         ...  \n",
      "111                  0  pre-market  \n",
      "112                  0  pre-market  \n",
      "113                  0  pre-market  \n",
      "114                  0  pre-market  \n",
      "115                  0  pre-market  \n",
      "\n",
      "[116 rows x 7 columns]\n",
      "      timestamp             value\n",
      "0    2024-12-01  166.477049861565\n",
      "1    2024-11-01  166.192164867385\n",
      "2    2024-10-01   166.49238129989\n",
      "3    2024-09-01  161.455215541659\n",
      "4    2024-08-01  164.174770420668\n",
      "..          ...               ...\n",
      "391  1992-05-01                 .\n",
      "392  1992-04-01                 .\n",
      "393  1992-03-01                 .\n",
      "394  1992-02-01                 .\n",
      "395  1992-01-01                 .\n",
      "\n",
      "[396 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "apikey = '####'\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey={apikey}&datatype=csv'\n",
    "df_fed = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey={apikey}&datatype=csv'\n",
    "df_inflation = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=UNEMPLOYMENT&apikey={apikey}&datatype=csv'\n",
    "df_unemployment = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={apikey}'\n",
    "r = requests.get(url)\n",
    "\n",
    "dataAnnual = r.json()['annualEarnings']\n",
    "dataQuarter = r.json()['quarterlyEarnings']\n",
    "df_annual_earnings = pd.DataFrame(dataAnnual)\n",
    "df_quarterly_earnings = pd.DataFrame(dataQuarter)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=ALL_COMMODITIES&interval=monthly&apikey={apikey}&datatype=csv'\n",
    "df_commodities = pd.read_csv(url)\n",
    "\n",
    "print(df_fed)\n",
    "print(df_inflation)\n",
    "print(df_unemployment)\n",
    "print(df_annual_earnings)\n",
    "print(df_quarterly_earnings)\n",
    "print(df_commodities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "0    Genpact Chief Accounting Officer Trades $226K ...   \n",
      "1    Portland General Electric  ( POR )  Q4 Earning...   \n",
      "2    US Stocks Set For A Volatile Start: Analyst Di...   \n",
      "3    Top Wall Street Forecasters Revamp Portland Ge...   \n",
      "4          Ameren  ( AEE )  Lags Q4 Earnings Estimates   \n",
      "..                                                 ...   \n",
      "670  One of Wall Street's favorite ways to control ...   \n",
      "671  HNA arm Bohai to soon start US$5 billion sale ...   \n",
      "672  Circuit Breaker Market Size is Expected to Rea...   \n",
      "673  Carlisle  ( CSL )  Q4 Earnings Beat Estimates,...   \n",
      "674  Process Automation And Instrumentation Market ...   \n",
      "\n",
      "                                                   url   time_published  \\\n",
      "0    https://www.benzinga.com/insights/news/25/02/4...  20250214T150435   \n",
      "1    https://www.zacks.com/stock/news/2415983/portl...  20250214T120503   \n",
      "2    https://www.benzinga.com/government/regulation...  20250214T110518   \n",
      "3    https://www.benzinga.com/25/02/43721760/top-wa...  20250214T072522   \n",
      "4    https://www.zacks.com/stock/news/2415821/amere...  20250213T224006   \n",
      "..                                                 ...              ...   \n",
      "670  https://www.cnbc.com/2024/02/08/one-of-wall-st...  20240208T125151   \n",
      "671  https://www.scmp.com/business/china-business/a...  20240208T023454   \n",
      "672  https://www.benzinga.com/pressreleases/24/02/g...  20240207T170000   \n",
      "673  https://www.zacks.com/stock/news/2222679/carli...  20240207T151800   \n",
      "674  https://www.benzinga.com/pressreleases/24/02/g...  20240207T151500   \n",
      "\n",
      "                     authors  \\\n",
      "0        [Benzinga Insights]   \n",
      "1    [Zacks Equity Research]   \n",
      "2           [Rishabh Mishra]   \n",
      "3               [Avi Kapoor]   \n",
      "4    [Zacks Equity Research]   \n",
      "..                       ...   \n",
      "670    [Cheryl Winokur Munk]   \n",
      "671              [Bloomberg]   \n",
      "672         [Globe Newswire]   \n",
      "673  [Zacks Equity Research]   \n",
      "674         [Globe Newswire]   \n",
      "\n",
      "                                               summary  \\\n",
      "0    Making a noteworthy insider sell on February 1...   \n",
      "1    Portland General Electric (POR) delivered earn...   \n",
      "2    U.S. stock futures traded close to the flatlin...   \n",
      "3    Portland General Electric Company POR will rel...   \n",
      "4    Ameren (AEE) delivered earnings and revenue su...   \n",
      "..                                                 ...   \n",
      "670  The FTC wants to ban non-compete agreements in...   \n",
      "671  Bohai Leasing, a Shenzhen-listed arm of failed...   \n",
      "672  Wilmington, Delaware, United States, Feb. 07, ...   \n",
      "673  Carlisle (CSL) fourth-quarter 2023 results ben...   \n",
      "674  Jersey City, New Jersey, Feb. 07, 2024 ( GLOBE...   \n",
      "\n",
      "                                          banner_image  \\\n",
      "0    https://cdn.benzinga.com/files/images/story/20...   \n",
      "1    https://staticx-tuner.zacks.com/images/default...   \n",
      "2    https://cdn.benzinga.com/files/images/story/20...   \n",
      "3    https://editorial-assets.benzinga.com/wp-conte...   \n",
      "4    https://staticx-tuner.zacks.com/images/default...   \n",
      "..                                                 ...   \n",
      "670  https://image.cnbcfm.com/api/v1/image/10723662...   \n",
      "671  https://cdn.i-scmp.com/sites/default/files/d8/...   \n",
      "672  https://www.benzinga.com/next-assets/images/sc...   \n",
      "673  https://staticx-tuner.zacks.com/images/default...   \n",
      "674  https://www.benzinga.com/next-assets/images/sc...   \n",
      "\n",
      "                       source category_within_source     source_domain  \\\n",
      "0                    Benzinga                Trading  www.benzinga.com   \n",
      "1            Zacks Commentary                    n/a     www.zacks.com   \n",
      "2                    Benzinga                Markets  www.benzinga.com   \n",
      "3                    Benzinga                Markets  www.benzinga.com   \n",
      "4            Zacks Commentary                    n/a     www.zacks.com   \n",
      "..                        ...                    ...               ...   \n",
      "670                      CNBC               Top News      www.cnbc.com   \n",
      "671  South China Morning Post              Companies      www.scmp.com   \n",
      "672                  Benzinga                General  www.benzinga.com   \n",
      "673          Zacks Commentary                    n/a     www.zacks.com   \n",
      "674                  Benzinga                General  www.benzinga.com   \n",
      "\n",
      "                                                topics  \\\n",
      "0    [{'topic': 'Earnings', 'relevance_score': '0.9...   \n",
      "1    [{'topic': 'Financial Markets', 'relevance_sco...   \n",
      "2    [{'topic': 'Life Sciences', 'relevance_score':...   \n",
      "3    [{'topic': 'Financial Markets', 'relevance_sco...   \n",
      "4    [{'topic': 'Financial Markets', 'relevance_sco...   \n",
      "..                                                 ...   \n",
      "670  [{'topic': 'Life Sciences', 'relevance_score':...   \n",
      "671  [{'topic': 'Real Estate & Construction', 'rele...   \n",
      "672  [{'topic': 'Technology', 'relevance_score': '0...   \n",
      "673  [{'topic': 'Earnings', 'relevance_score': '0.9...   \n",
      "674  [{'topic': 'Life Sciences', 'relevance_score':...   \n",
      "\n",
      "     overall_sentiment_score overall_sentiment_label  \\\n",
      "0                   0.242835        Somewhat-Bullish   \n",
      "1                   0.150400        Somewhat-Bullish   \n",
      "2                   0.192551        Somewhat-Bullish   \n",
      "3                   0.118315                 Neutral   \n",
      "4                   0.172792        Somewhat-Bullish   \n",
      "..                       ...                     ...   \n",
      "670                 0.120522                 Neutral   \n",
      "671                 0.061619                 Neutral   \n",
      "672                 0.442516                 Bullish   \n",
      "673                 0.092527                 Neutral   \n",
      "674                 0.312789        Somewhat-Bullish   \n",
      "\n",
      "                                      ticker_sentiment  \n",
      "0    [{'ticker': 'GE', 'relevance_score': '0.056143...  \n",
      "1    [{'ticker': 'GE', 'relevance_score': '0.328236...  \n",
      "2    [{'ticker': 'META', 'relevance_score': '0.0715...  \n",
      "3    [{'ticker': 'GE', 'relevance_score': '0.324495...  \n",
      "4    [{'ticker': 'AEE', 'relevance_score': '0.40133...  \n",
      "..                                                 ...  \n",
      "670  [{'ticker': 'GIS', 'relevance_score': '0.02849...  \n",
      "671  [{'ticker': 'CRON', 'relevance_score': '0.1125...  \n",
      "672  [{'ticker': 'GOOG', 'relevance_score': '0.0360...  \n",
      "673  [{'ticker': 'MMM', 'relevance_score': '0.12779...  \n",
      "674  [{'ticker': 'GIC', 'relevance_score': '0.10453...  \n",
      "\n",
      "[675 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={apikey}&sort=LATEST&limit=1000'\n",
    "r = requests.get(url)\n",
    "data = r.json()['feed']\n",
    "df_sentiments = pd.DataFrame(data)\n",
    "print(df_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lag 1'] = df['Close'].shift(1)\n",
    "df['7 Day Avg'] = df['Close'].rolling(window=7).mean()\n",
    "df['Daily Returns'] = (df['Close'] - df['Open']) / df['Open']\n",
    "df['Price to Volume Ratio'] = df['Close'] / df['Volume']\n",
    "df['Day of the Week'] = df['Date'].dt.dayofweek\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "df['Daily Return'] = df['Close'].pct_change()\n",
    "df['Volatility'] = df['High'] - df['Low']\n",
    "df['Price Volume Interaction'] = df['Daily Return'] * df['Volume']\n",
    "df['RSI * Volume'] = df['RSI'] * df['Volume']\n",
    "df['MACD / Bollinger Band Width'] = df['MACD'] / df['BB_Width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finlight key = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Genpact Chief Accounting Officer Trades $226K ...   \n",
      "1  Portland General Electric  ( POR )  Q4 Earning...   \n",
      "2  US Stocks Set For A Volatile Start: Analyst Di...   \n",
      "3  Top Wall Street Forecasters Revamp Portland Ge...   \n",
      "4        Ameren  ( AEE )  Lags Q4 Earnings Estimates   \n",
      "\n",
      "                                             summary  overall_sentiment_score  \\\n",
      "0  Making a noteworthy insider sell on February 1...                 0.242835   \n",
      "1  Portland General Electric (POR) delivered earn...                 0.150400   \n",
      "2  U.S. stock futures traded close to the flatlin...                 0.192551   \n",
      "3  Portland General Electric Company POR will rel...                 0.118315   \n",
      "4  Ameren (AEE) delivered earnings and revenue su...                 0.172792   \n",
      "\n",
      "  overall_sentiment_label                                   ticker_sentiment  \\\n",
      "0        Somewhat-Bullish  [{'ticker': 'GE', 'relevance_score': '0.056143...   \n",
      "1        Somewhat-Bullish  [{'ticker': 'GE', 'relevance_score': '0.328236...   \n",
      "2        Somewhat-Bullish  [{'ticker': 'META', 'relevance_score': '0.0715...   \n",
      "3                 Neutral  [{'ticker': 'GE', 'relevance_score': '0.324495...   \n",
      "4        Somewhat-Bullish  [{'ticker': 'AEE', 'relevance_score': '0.40133...   \n",
      "\n",
      "         Date  \n",
      "0  2025-02-14  \n",
      "1  2025-02-14  \n",
      "2  2025-02-14  \n",
      "3  2025-02-14  \n",
      "4  2025-02-13  \n"
     ]
    }
   ],
   "source": [
    "dropColumns = [1,3,5,6,7,8,9]\n",
    "\n",
    "df_sentiments.drop(df_sentiments.columns[dropColumns], axis = 1, inplace=True)\n",
    "\n",
    "df_sentiments['date'] = pd.to_datetime(df_sentiments['time_published'], format = '%Y%m%dT%H%M%S')\n",
    "df_sentiments['Date'] = df_sentiments['date'].dt.date\n",
    "\n",
    "dropColumns = [1, 6]\n",
    "df_sentiments.drop(df_sentiments.columns[dropColumns], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "print(df_sentiments.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I figured this out kind of, just not sure how to incorporate this into my data. I will look into this in the future but for now we'll leave it. Will also figure out finlight api later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Open        High         Low       Close  \\\n",
      "Date                                                                        \n",
      "1998-12-22 00:00:00-05:00   14.622297   14.671531   14.622297   14.671531   \n",
      "1998-12-23 00:00:00-05:00   14.779837   14.966924   14.779837   14.927537   \n",
      "1998-12-24 00:00:00-05:00   15.065394   15.134320   15.016160   15.124474   \n",
      "1998-12-28 00:00:00-05:00   15.124485   15.213105   15.124485   15.203259   \n",
      "1998-12-29 00:00:00-05:00   15.282026   15.419879   15.282026   15.419879   \n",
      "...                               ...         ...         ...         ...   \n",
      "2025-02-10 00:00:00-05:00  138.220001  138.610001  137.630005  138.559998   \n",
      "2025-02-11 00:00:00-05:00  138.100006  138.679993  137.699997  138.610001   \n",
      "2025-02-12 00:00:00-05:00  136.630005  138.240005  136.410004  137.750000   \n",
      "2025-02-13 00:00:00-05:00  137.740005  138.250000  137.309998  137.889999   \n",
      "2025-02-14 00:00:00-05:00  138.080002  138.250000  137.389999  137.550003   \n",
      "\n",
      "                            Volume  Dividends  Stock Splits  Capital Gains  \n",
      "Date                                                                        \n",
      "1998-12-22 00:00:00-05:00      600        0.0           0.0            0.0  \n",
      "1998-12-23 00:00:00-05:00      900        0.0           0.0            0.0  \n",
      "1998-12-24 00:00:00-05:00     1400        0.0           0.0            0.0  \n",
      "1998-12-28 00:00:00-05:00     7400        0.0           0.0            0.0  \n",
      "1998-12-29 00:00:00-05:00     1400        0.0           0.0            0.0  \n",
      "...                            ...        ...           ...            ...  \n",
      "2025-02-10 00:00:00-05:00  5446400        0.0           0.0            0.0  \n",
      "2025-02-11 00:00:00-05:00  5787700        0.0           0.0            0.0  \n",
      "2025-02-12 00:00:00-05:00  7694300        0.0           0.0            0.0  \n",
      "2025-02-13 00:00:00-05:00  8539100        0.0           0.0            0.0  \n",
      "2025-02-14 00:00:00-05:00  5800000        0.0           0.0            0.0  \n",
      "\n",
      "[6578 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "SectorTicker = {'COMMUNICATION SERVICES': 'XLC',\n",
    "              'CONSUMER DISCRETIONARY': 'XLY',\n",
    "              'CONSUMER STAPLES': 'XLP',\n",
    "              'ENERGY': 'XLE',\n",
    "              'FINANCIALS': 'XLF',\n",
    "              'HEALTHCARE': 'XLV',\n",
    "              'INDUSTRIALS': 'XLI',\n",
    "              'MATERIALS': 'XLB',\n",
    "              'REAL ESTATE': 'VNQ',\n",
    "              'TECHNOLOGY': 'XLK',\n",
    "              'UTILITIES': 'XLU'}\n",
    "\n",
    "\n",
    "info = yf.Ticker(ticker).get_info()\n",
    "sectorName = info['sector'].upper()\n",
    "sectorSymbol = SectorTicker[sectorName]\n",
    "\n",
    "df_sector = yf.Ticker(sectorSymbol).history(period=\"max\")\n",
    "df_sector.sort_values(by = 'Date', inplace = True)\n",
    "\n",
    "print(df_sector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Cleaning\n",
    "Just to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           datetime64[ns]\n",
      "Open                                  float32\n",
      "High                                  float32\n",
      "Low                                   float32\n",
      "Close                                 float32\n",
      "Volume                                  int32\n",
      "Dividends                             float32\n",
      "Stock Splits                          float32\n",
      "SMA_10                                float64\n",
      "SMA_50                                float64\n",
      "SMA_100                               float64\n",
      "SMA_200                               float64\n",
      "EMA_10                                float64\n",
      "EMA_50                                float64\n",
      "EMA_100                               float64\n",
      "EMA_200                               float64\n",
      "RSI                                   float64\n",
      "MACD                                  float64\n",
      "MACD_signal                           float64\n",
      "Stochastic                            float64\n",
      "Bollinger_High                        float64\n",
      "Bollinger_Low                         float64\n",
      "ATR                                   float64\n",
      "OBV                                     int64\n",
      "Chaikin                               float64\n",
      "ADX                                   float64\n",
      "SAR_Down                              float64\n",
      "SAR_Up                                float64\n",
      "MA_Crossover                          float64\n",
      "BB_Width                              float64\n",
      "Lag 1                                 float32\n",
      "7 Day Avg                             float64\n",
      "Daily Returns                         float32\n",
      "Price to Volume Ratio                 float64\n",
      "Day of the Week                         int32\n",
      "Quarter                                 int32\n",
      "Daily Return                          float32\n",
      "Volatility                            float32\n",
      "Price Volume Interaction              float64\n",
      "RSI * Volume                          float64\n",
      "MACD / Bollinger Band Width           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           0\n",
      "Open                           0\n",
      "High                           0\n",
      "Low                            0\n",
      "Close                          0\n",
      "Volume                         0\n",
      "Dividends                      0\n",
      "Stock Splits                   0\n",
      "SMA_10                         0\n",
      "SMA_50                         0\n",
      "SMA_100                        0\n",
      "SMA_200                        0\n",
      "EMA_10                         0\n",
      "EMA_50                         0\n",
      "EMA_100                        0\n",
      "EMA_200                        0\n",
      "RSI                            0\n",
      "MACD                           0\n",
      "MACD_signal                    0\n",
      "Stochastic                     0\n",
      "Bollinger_High                 0\n",
      "Bollinger_Low                  0\n",
      "ATR                            0\n",
      "OBV                            0\n",
      "Chaikin                        0\n",
      "ADX                            0\n",
      "SAR_Down                       0\n",
      "SAR_Up                         0\n",
      "MA_Crossover                   0\n",
      "BB_Width                       0\n",
      "Lag 1                          0\n",
      "7 Day Avg                      0\n",
      "Daily Returns                  0\n",
      "Price to Volume Ratio          0\n",
      "Day of the Week                0\n",
      "Quarter                        0\n",
      "Daily Return                   0\n",
      "Volatility                     0\n",
      "Price Volume Interaction       0\n",
      "RSI * Volume                   0\n",
      "MACD / Bollinger Band Width    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           0\n",
      "Open                           0\n",
      "High                           0\n",
      "Low                            0\n",
      "Close                          0\n",
      "Volume                         0\n",
      "Dividends                      0\n",
      "Stock Splits                   0\n",
      "SMA_10                         0\n",
      "SMA_50                         0\n",
      "SMA_100                        0\n",
      "SMA_200                        0\n",
      "EMA_10                         0\n",
      "EMA_50                         0\n",
      "EMA_100                        0\n",
      "EMA_200                        0\n",
      "RSI                            0\n",
      "MACD                           0\n",
      "MACD_signal                    0\n",
      "Stochastic                     0\n",
      "Bollinger_High                 0\n",
      "Bollinger_Low                  0\n",
      "ATR                            0\n",
      "OBV                            0\n",
      "Chaikin                        0\n",
      "ADX                            0\n",
      "SAR_Down                       0\n",
      "SAR_Up                         0\n",
      "MA_Crossover                   0\n",
      "BB_Width                       0\n",
      "Lag 1                          0\n",
      "7 Day Avg                      0\n",
      "Daily Returns                  0\n",
      "Price to Volume Ratio          1\n",
      "Day of the Week                0\n",
      "Quarter                        0\n",
      "Daily Return                   0\n",
      "Volatility                     0\n",
      "Price Volume Interaction       0\n",
      "RSI * Volume                   0\n",
      "MACD / Bollinger Band Width    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isin([np.inf, -np.inf]).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Price to Volume Ratio'] = df['Price to Volume Ratio'].replace([np.inf, -np.inf], np.nan) \n",
    "df['MACD / Bollinger Band Width'] = df['MACD / Bollinger Band Width'].replace([np.inf, -np.inf], np.nan) \n",
    "df['Daily Returns'] = df['Daily Returns'].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiscalDateEnding    datetime64[ns]\n",
      "reportedEPS                float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_annual_earnings.columns[1] \n",
    "df_annual_earnings[column] = pd.to_numeric(df_annual_earnings[column], downcast='float')\n",
    "\n",
    "column = df_annual_earnings.columns[0]\n",
    "df_annual_earnings[column] = pd.to_datetime(df_annual_earnings[column])\n",
    "print(df_annual_earnings.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_commodities.columns[0]\n",
    "df_commodities[column] = pd.to_datetime(df_commodities[column])\n",
    "\n",
    "column = df_commodities.columns[1] \n",
    "df_commodities[column] = pd.to_numeric(df_commodities[column], errors='coerce', downcast='float')\n",
    "print(df_commodities.dtypes)\n",
    "df_commodities.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_fed.columns[0]\n",
    "df_fed[column] = pd.to_datetime(df_fed[column])\n",
    "print(df_fed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_inflation.columns[0]\n",
    "df_inflation[column] = pd.to_datetime(df_inflation[column])\n",
    "print(df_inflation.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quarterly_earnings.drop(columns='fiscalDateEnding', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reportedDate          datetime64[ns]\n",
      "reportedEPS                  float32\n",
      "estimatedEPS                 float32\n",
      "surprise                     float32\n",
      "surprisePercentage           float32\n",
      "reportTime                      int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_quarterly_earnings.columns[0]\n",
    "df_quarterly_earnings[column] = pd.to_datetime(df_quarterly_earnings[column])\n",
    "\n",
    "columns = list(df_quarterly_earnings.columns)\n",
    "\n",
    "for num in range(1, len(columns) - 1):\n",
    "    df_quarterly_earnings[columns[num]] = pd.to_numeric(df_quarterly_earnings[columns[num]], downcast='float')\n",
    "df_quarterly_earnings['reportTime'] = df_quarterly_earnings['reportTime'].apply(lambda x: 1 if x == 'post-market' else -1 if x == 'pre-market' else 0)\n",
    "df_quarterly_earnings['reportTime'] = pd.to_numeric(df_quarterly_earnings['reportTime'], downcast='integer')\n",
    "print(df_quarterly_earnings.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open             float64\n",
      "High             float64\n",
      "Low              float64\n",
      "Close            float64\n",
      "Volume             int64\n",
      "Dividends        float64\n",
      "Stock Splits     float64\n",
      "Capital Gains    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_sector.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_unemployment.columns[0]\n",
    "df_unemployment[column] = pd.to_datetime(df_unemployment[column])\n",
    "print(df_unemployment.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_sentiment_score        Date\n",
      "0                 0.242835  2025-02-14\n",
      "1                 0.150400  2025-02-14\n",
      "2                 0.192551  2025-02-14\n",
      "3                 0.118315  2025-02-14\n",
      "4                 0.172792  2025-02-13\n"
     ]
    }
   ],
   "source": [
    "df_overall_sentiments = df_sentiments.copy()\n",
    "columns = [0, 1, 3, 4]\n",
    "df_overall_sentiments.drop(df_overall_sentiments.columns[columns], axis = 1, inplace=True)\n",
    "print(df_overall_sentiments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'Date'\n",
    "df_annual_earnings.rename(columns = {'fiscalDateEnding': date, 'reportedEPS': 'AnnualEarnings'}, errors='ignore', inplace = True)\n",
    "df_commodities.rename(columns={'timestamp': date, 'value': 'Commodities_Index'}, errors='ignore', inplace=True)\n",
    "df_fed.rename(columns = {'timestamp': date, 'value':'Fed_Rate'}, errors='ignore', inplace=True)\n",
    "df_inflation.rename(columns = {'timestamp': date, 'value':'Inflation'}, errors='ignore', inplace=True)\n",
    "df_quarterly_earnings.rename(columns = {'reportedDate': date, 'reportedEPS': 'QuarterEarnings', 'estimatedEPS': 'EstimatedQuarterEarnings', 'surprise': 'QuarterSurprise', 'surprisePercentage': 'QuarterSurprisePercentage', 'reportTime': 'QuarterReportTime'}, errors='ignore', inplace = True)\n",
    "df_sector.rename(columns = {'timestamp': date, 'Open': 'SectorOpen', 'High':'SectorHigh', 'Low':'SectorLow', 'Close': 'SectorClose', 'Volume':'SectorVolume'}, errors='ignore', inplace = True)\n",
    "df_unemployment.rename(columns = {'timestamp': date, 'value':'UnemploymentRate'}, errors='ignore', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropColumns = ['Dividends','Stock Splits', 'Capital Gains']\n",
    "df_sector = df_sector.reset_index()\n",
    "df_sector.drop(columns=dropColumns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_sector[\"Date\"].dt.tz is not None:\n",
    "    df_sector[\"Date\"] = df_sector[\"Date\"].dt.tz_convert(None)\n",
    "else:\n",
    "    df_sector[\"Date\"] = df_sector[\"Date\"].dt.tz_localize(\"UTC\")  \n",
    "    df_sector[\"Date\"] = df_sector[\"Date\"].dt.tz_convert(None)   \n",
    "df_sector[\"Date\"] = df_sector[\"Date\"].dt.normalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserting is a success!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "server = 'dahomey.database.windows.net'\n",
    "database = 'Stock Data'\n",
    "username = 'ttshiamala'\n",
    "password = '####'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Connect to Azure SQL\n",
    "connection_string = f\"DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}\"\n",
    "conn = pyodbc.connect(connection_string)\n",
    "\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "df.to_sql('Stock', engine, if_exists = 'replace', index=False)\n",
    "\n",
    "print('Data inserting is a success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [df_annual_earnings, df_commodities, df_fed, df_inflation, df_overall_sentiments, df_quarterly_earnings, df_sector,  df_unemployment]\n",
    "list_tables = ['AnnualEarnings', 'Commodities', 'FederalFundsRate', 'Inflation', 'Sentiments', 'QuarterlyEarnings', 'SectorPrices', 'Unemployment']\n",
    "\n",
    "conn = pyodbc.connect(connection_string)\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "for i, dataFrame in enumerate(list_df):\n",
    "    dataFrame.to_sql(list_tables[i], engine, if_exists = 'replace', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Query and Loading\n",
    "This query joins all the data tables that was collected and loaded onto the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Query.sql\", \"r\") as file:\n",
    "    query = file.read()\n",
    "\n",
    "conn = pyodbc.connect(connection_string)\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "finalDF = pd.read_sql(sql=query, con=engine, parse_dates=['Date'])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps is to make sure that the data makes sense. For example, with unemployment, not all values were added because of left join so I may have to revisit how I joined these tables. Need to account for days that the stock wasn't trading (commodities as well). Also have to double check sentiments. Need to also consider the case that there were multiple stories in one day (average). After that I have to clean the dataset once more and handle missing values and things of that nature. Actually now that I'm taking a closer look at it, there are some spurious tables. It may be because of the overall sentiments for example with multiple dates.\n",
    "\n",
    "I fixed it the dataset is almost perfect. Just need to clean it one more time then it is ready for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Datasets\n",
    "This is a big deal. The way we datasets and handle missing values will affect the machine learning algorithms. Some algorithms know how to handle missing values like random forests for example. Linear regression, for instance, don't know how to directly handle those situations. There is a plethora of ways we can handle the situation. Based on some research, forward fill may be a viable option. There are some downsides though. For example, quarterly earnings aren't released everyday, they're released quarterly. So, having them be present on overy row wouldn't make sense. A way we can fix is this to have a another feature calculating how many days since the anouncment of earnings to keep residual effects. I think what I'll do is use different methods to optimze performance different models. I'll do Random Forest, XGBoost, CNN, and LSTM as my main models. In the future I want to try Linear Regression, SVM, and Transformers. But Transformers require more data than I have. SVM is more for classification and less for continuous price prediction. Linear regression assumes there's a linear relationship, which seems unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used Power BI to visualize the stock and I noticed that the GE stock crashes a lot. It made me think if the type of stock affects the model. It turns out it does! So these are all the major things I have to consider when I'm creating these models.\n",
    "1. Which model to use\n",
    "2. How much data for the model\n",
    "3. Which features to use\n",
    "4. Dimensional Reduction algorithms?\n",
    "5. Handling Missing Values\n",
    "6. Which stock to analyze and sector and what type of features it's susceptible to \n",
    "7. Normalization\n",
    "8. time frame of the prediction\n",
    "- Are you predicting daily, weekly, or monthly prices? Different models perform better on different time horizons.\n",
    "9. data splitting\n",
    "10. overfitting and regularization\n",
    "- Apply techniques like early stopping, dropout (for neural networks), or L1/L2 regularization to prevent overfitting.\n",
    "11. Model Explainability\n",
    "- Use techniques like SHAP values or LIME to understand which factors drive predictions.\n",
    "12. Computer resources\n",
    "13. Hypertuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
