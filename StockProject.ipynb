{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Data Project \n",
    "Objective: Predict future stock prices (e.g., next day, week, or month) using historical data and external factors.\n",
    "\n",
    "Type of Prediction:\n",
    "\n",
    "Regression: Predict exact future prices.\n",
    "\n",
    "Classification: Predict price movement direction (up/down/stable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data Retreival From Alpha Vantage\n",
    "\n",
    "API Key to Alpha Vantage (source for data): HPUL5XC5C1RFHRAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp     open     high       low   close    volume\n",
      "6345  1999-11-01   98.500   98.810   96.3700   96.75   9551800\n",
      "6344  1999-11-02   96.750   96.810   93.6900   94.81  11105400\n",
      "6343  1999-11-03   95.870   95.940   93.5000   94.37  10369100\n",
      "6342  1999-11-04   94.440   94.440   90.0000   91.56  16697600\n",
      "6341  1999-11-05   92.750   92.940   90.1900   90.25  13737600\n",
      "...          ...      ...      ...       ...     ...       ...\n",
      "4     2025-01-16  219.690  222.680  217.3800  222.66   3329060\n",
      "3     2025-01-17  225.955  225.955  223.6400  224.79   5506837\n",
      "2     2025-01-21  224.990  227.450  222.8302  224.26   3982203\n",
      "1     2025-01-22  221.980  224.400  220.3500  223.26   4759490\n",
      "0     2025-01-23  223.940  226.040  223.1500  226.04   3619651\n",
      "\n",
      "[6346 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "function = 'TIME_SERIES_DAILY'\n",
    "symbol = 'IBM'\n",
    "outputsize = 'full'\n",
    "datatype = 'csv'\n",
    "apikey = 'HPUL5XC5C1RFHRAQ'\n",
    "\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function={function}&symbol={symbol}&outputsize={outputsize}&apikey={apikey}&datatype={datatype}'\n",
    "df = pd.read_csv(url)\n",
    "df.sort_values(by = 'timestamp', inplace = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning \n",
    "\n",
    "Using Data Wrangler the data seems pretty clean but to ensure data integrity, I'm going to use some data cleaning methods using pandas.\n",
    "\n",
    "We have to ensure there's no missing data, a consistent format, removal of unwanted observations, incorrect data, and removal of duplicates. We don't need to manage outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removal of rows with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensures that the dates are in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ensures that there is no wrong data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "columns.remove('timestamp')\n",
    "columns.remove('volume')\n",
    "\n",
    "# Ensure that the columns are of the correct data type\n",
    "for column in columns:\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce', downcast='float')\n",
    "\n",
    "df['volume'] = pd.to_numeric(df['volume'], errors='coerce', downcast='integer')\n",
    "\n",
    "#Make sure that all numbers are positive, if not make it positive\n",
    "columns.append('volume')\n",
    "for column in columns:\n",
    "    for i in df.index:\n",
    "        if df.loc[i, column] < 0:\n",
    "            df.loc[i, column] = df.loc[i, column] * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Removing duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transforming\n",
    "\n",
    "* Slight formating for column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date       Open       High        Low      Close    Volume\n",
      "6345 1999-11-01  98.500000  98.809998  96.370003  96.750000   9551800\n",
      "6344 1999-11-02  96.750000  96.809998  93.690002  94.809998  11105400\n",
      "6343 1999-11-03  95.870003  95.940002  93.500000  94.370003  10369100\n",
      "6342 1999-11-04  94.440002  94.440002  90.000000  91.559998  16697600\n",
      "6341 1999-11-05  92.750000  92.940002  90.190002  90.250000  13737600\n"
     ]
    }
   ],
   "source": [
    "df.rename(columns = {'timestamp':'Date', 'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'}, inplace = True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading \n",
    "\n",
    "* Loading the data into the SQL Database provided by the cloud service Azure. This only needs to happen once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'server = \\'dahomey.database.windows.net\\'\\ndatabase = \\'Stock Data\\'\\nusername = \\'ttshiamala\\'\\npassword = \\'Bear8486!?\\'\\ndriver = \\'{ODBC Driver 18 for SQL Server}\\'\\n\\n# Connect to Azure SQL\\nconnection_string = f\"DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}\"\\nconn = pyodbc.connect(connection_string)\\ncursor = conn.cursor()\\nprint(\"Connected to Azure SQL Database!\")\\ntry:\\n    cursor.execute(\\'TRUNCATE TABLE StockPrices\\')\\n    conn.commit()\\n    for index, row in df.iterrows():\\n        cursor.execute(\\n            \"\"\"\\n            INSERT INTO StockPrices ([Date], [Open], [High], [Low], [Close], [Volume]) \\n            VALUES (?, ?, ?, ?, ?, ?)\\n            \"\"\",\\n            row.Date, row.Open, row.High, row.Low, row.Close, row.Volume\\n    )\\nexcept pyodbc.IntegrityError as e:\\n    print(e)\\n\\n\\nconn.commit()\\ncursor.close()\\nconn.close()\\nprint(\\'Data inserting is a success!\\')'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''server = 'dahomey.database.windows.net'\n",
    "database = 'Stock Data'\n",
    "username = 'ttshiamala'\n",
    "password = 'Bear8486!?'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Connect to Azure SQL\n",
    "connection_string = f\"DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}\"\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "print(\"Connected to Azure SQL Database!\")\n",
    "try:\n",
    "    cursor.execute('TRUNCATE TABLE StockPrices')\n",
    "    conn.commit()\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO StockPrices ([Date], [Open], [High], [Low], [Close], [Volume]) \n",
    "            VALUES (?, ?, ?, ?, ?, ?)\n",
    "            \"\"\",\n",
    "            row.Date, row.Open, row.High, row.Low, row.Close, row.Volume\n",
    "    )\n",
    "except pyodbc.IntegrityError as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print('Data inserting is a success!')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data \n",
    "\n",
    "* This is where I will be collecting more data for the AI to learn from as well as do some feature engineering. A lot of repeated steps for previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ex_dividend_date declaration_date record_date payment_date amount\n",
      "0         2024-11-12       2024-10-30  2024-11-12   2024-12-10   1.67\n",
      "1         2024-08-09       2024-07-29  2024-08-09   2024-09-10   1.67\n",
      "2         2024-05-09       2024-04-30  2024-05-10   2024-06-10   1.67\n",
      "3         2024-02-08       2024-01-30  2024-02-09   2024-03-09   1.66\n",
      "4         2023-11-09       2023-10-30  2023-11-10   2023-12-09   1.66\n",
      "..               ...              ...         ...          ...    ...\n",
      "99        2000-02-08             None        None         None   0.12\n",
      "100       1999-11-08             None        None         None   0.12\n",
      "101       1999-08-06             None        None         None   0.12\n",
      "102       1999-05-06             None        None         None   0.24\n",
      "103       1999-02-08             None        None         None   0.22\n",
      "\n",
      "[104 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = f'https://www.alphavantage.co/query?function=DIVIDENDS&symbol={symbol}&apikey={apikey}'\n",
    "\n",
    "r = requests.get(url)\n",
    "data = r.json()[\"data\"]\n",
    "\n",
    "df_div = pd.DataFrame(data)\n",
    "print(df_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ex_dividend_date amount\n",
      "0       2024-11-12   1.67\n",
      "1       2024-08-09   1.67\n",
      "2       2024-05-09   1.67\n",
      "3       2024-02-08   1.66\n",
      "4       2023-11-09   1.66\n"
     ]
    }
   ],
   "source": [
    "df_div.drop(df_div.columns[1:4], axis = 1, inplace=True)\n",
    "print(df_div.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  effective_date split_factor\n",
      "0     2021-11-04       1.0460\n",
      "1     1999-05-27       2.0000\n"
     ]
    }
   ],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=SPLITS&symbol={symbol}&apikey={apikey}'\n",
    "r = requests.get(url)\n",
    "data = r.json()[\"data\"]\n",
    "df_split = pd.DataFrame(data)\n",
    "\n",
    "print(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_dividend_date    datetime64[ns]\n",
      "amount                     float32\n",
      "dtype: object\n",
      "effective_date    datetime64[ns]\n",
      "split_factor             float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_div['ex_dividend_date'] = pd.to_datetime(df_div['ex_dividend_date'])\n",
    "df_div['amount'] = pd.to_numeric(df_div['amount'], errors='coerce',downcast='float')\n",
    "\n",
    "df_split['effective_date'] = pd.to_datetime(df_split['effective_date'])\n",
    "df_split['split_factor'] = pd.to_numeric(df_split['split_factor'], errors='coerce', downcast ='float')\n",
    "\n",
    "print(df_div.dtypes)\n",
    "print(df_split.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date        Open        High         Low       Close   Volume  \\\n",
      "4 2025-01-16  219.690002  222.679993  217.380005  222.660004  3329060   \n",
      "3 2025-01-17  225.955002  225.955002  223.639999  224.789993  5506837   \n",
      "2 2025-01-21  224.990005  227.449997  222.830200  224.259995  3982203   \n",
      "1 2025-01-22  221.979996  224.399994  220.350006  223.259995  4759490   \n",
      "0 2025-01-23  223.940002  226.039993  223.149994  226.039993  3619651   \n",
      "\n",
      "       SMA_10      SMA_50   SMA_100    SMA_200  \n",
      "4  220.998999  221.585001  219.1539  198.84455  \n",
      "3  221.483998  221.954400  219.4408  199.02410  \n",
      "2  221.644998  222.288200  219.7036  199.19090  \n",
      "1  221.703998  222.481400  219.9489  199.36750  \n",
      "0  221.911996  222.728400  220.2247  199.55200  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['SMA_10'] = df['Close'].rolling(window = 10).mean()\n",
    "df['SMA_50'] = df['Close'].rolling(window = 50).mean()\n",
    "df['SMA_100'] = df['Close'].rolling(window = 100).mean()\n",
    "df['SMA_200'] = df['Close'].rolling(window = 200).mean()\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date        Open        High         Low       Close   Volume  \\\n",
      "4 2025-01-16  219.690002  222.679993  217.380005  222.660004  3329060   \n",
      "3 2025-01-17  225.955002  225.955002  223.639999  224.789993  5506837   \n",
      "2 2025-01-21  224.990005  227.449997  222.830200  224.259995  3982203   \n",
      "1 2025-01-22  221.979996  224.399994  220.350006  223.259995  4759490   \n",
      "0 2025-01-23  223.940002  226.039993  223.149994  226.039993  3619651   \n",
      "\n",
      "       SMA_10      SMA_50   SMA_100    SMA_200      EMA_10      EMA_50  \\\n",
      "4  220.998999  221.585001  219.1539  198.84455  221.023838  221.606832   \n",
      "3  221.483998  221.954400  219.4408  199.02410  221.708593  221.731662   \n",
      "2  221.644998  222.288200  219.7036  199.19090  222.172484  221.830812   \n",
      "1  221.703998  222.481400  219.9489  199.36750  222.370213  221.886859   \n",
      "0  221.911996  222.728400  220.2247  199.55200  223.037446  222.049727   \n",
      "\n",
      "      EMA_100     EMA_200  \n",
      "4  215.850680  152.958646  \n",
      "3  216.027696  153.030568  \n",
      "2  216.190712  153.101887  \n",
      "1  216.330698  153.172133  \n",
      "0  216.522961  153.245093  \n"
     ]
    }
   ],
   "source": [
    "df['EMA_10'] = df['Close'].ewm(span=10).mean()\n",
    "df['EMA_50'] = df['Close'].ewm(span=50).mean()\n",
    "df['EMA_100'] = df['Close'].ewm(span=100).mean()\n",
    "df['EMA_200'] = df['Close'].ewm(span=2000).mean()\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date        Open        High         Low       Close   Volume  \\\n",
      "4 2025-01-16  219.690002  222.679993  217.380005  222.660004  3329060   \n",
      "3 2025-01-17  225.955002  225.955002  223.639999  224.789993  5506837   \n",
      "2 2025-01-21  224.990005  227.449997  222.830200  224.259995  3982203   \n",
      "1 2025-01-22  221.979996  224.399994  220.350006  223.259995  4759490   \n",
      "0 2025-01-23  223.940002  226.039993  223.149994  226.039993  3619651   \n",
      "\n",
      "       SMA_10      SMA_50   SMA_100    SMA_200  ...  Bollinger_High  \\\n",
      "4  220.998999  221.585001  219.1539  198.84455  ...      227.296598   \n",
      "3  221.483998  221.954400  219.4408  199.02410  ...      226.231592   \n",
      "2  221.644998  222.288200  219.7036  199.19090  ...      226.490727   \n",
      "1  221.703998  222.481400  219.9489  199.36750  ...      226.410646   \n",
      "0  221.911996  222.728400  220.2247  199.55200  ...      226.854383   \n",
      "\n",
      "   Bollinger_Low       ATR        OBV   Chaikin        ADX    SAR_Down  \\\n",
      "4     216.753400  4.393333  362926052 -0.013994  16.206601  223.404846   \n",
      "3     217.400406  4.314881  368432889 -0.016474  15.591217  223.404846   \n",
      "2     217.550271  4.336660  364450686  0.018172  15.386641  223.404846   \n",
      "1     217.564351  4.316184  359691196  0.072147  14.469365  223.404846   \n",
      "0     217.388613  4.214313  363310847  0.224645  14.038304  223.404846   \n",
      "\n",
      "       SAR_Up  MA_Crossover   BB_Width  \n",
      "4  214.610001     -0.586002  10.543198  \n",
      "3  214.771393     -0.470402   8.831185  \n",
      "2  215.218735     -0.643202   8.940456  \n",
      "1  215.952606     -0.777402   8.846295  \n",
      "0  216.642456     -0.816403   9.465770  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "import ta\n",
    "\n",
    "df['RSI'] = ta.momentum.rsi(df['Close'], window = 14, fillna = True)\n",
    "df['MACD'] = ta.trend.macd(df['Close'], fillna = True)\n",
    "df['MACD_signal'] = ta.trend.macd_signal(df['Close'], fillna = True)\n",
    "df['Stochastic'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['Bollinger_High'] = ta.volatility.bollinger_hband(df['Close'], fillna = True)\n",
    "df['Bollinger_Low'] = ta.volatility.bollinger_lband(df['Close'], fillna = True)\n",
    "df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'], fillna = True)\n",
    "df['Chaikin'] = ta.volume.chaikin_money_flow(df['High'], df['Low'], df['Close'], df['Volume'], fillna = True)\n",
    "df['ADX'] = ta.trend.adx(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['SAR_Down'] = ta.trend.psar_down(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['SAR_Up'] = ta.trend.psar_up(df['High'], df['Low'], df['Close'], fillna = True)\n",
    "df['MA_Crossover'] = df['SMA_10'] - df['SMA_50']\n",
    "df['BB_Width'] = df['Bollinger_High'] - df['Bollinger_Low']\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp  value\n",
      "0      2025-01-22   4.33\n",
      "1      2025-01-21   4.33\n",
      "2      2025-01-20   4.33\n",
      "3      2025-01-19   4.33\n",
      "4      2025-01-18   4.33\n",
      "...           ...    ...\n",
      "25769  1954-07-05   0.88\n",
      "25770  1954-07-04   1.25\n",
      "25771  1954-07-03   1.25\n",
      "25772  1954-07-02   1.25\n",
      "25773  1954-07-01   1.13\n",
      "\n",
      "[25774 rows x 2 columns]\n",
      "       timestamp    value\n",
      "0     2024-12-01  315.605\n",
      "1     2024-11-01  315.493\n",
      "2     2024-10-01  315.664\n",
      "3     2024-09-01  315.301\n",
      "4     2024-08-01  314.796\n",
      "...          ...      ...\n",
      "1339  1913-05-01    9.700\n",
      "1340  1913-04-01    9.800\n",
      "1341  1913-03-01    9.800\n",
      "1342  1913-02-01    9.800\n",
      "1343  1913-01-01    9.800\n",
      "\n",
      "[1344 rows x 2 columns]\n",
      "      timestamp  value\n",
      "0    2024-12-01    4.1\n",
      "1    2024-11-01    4.2\n",
      "2    2024-10-01    4.1\n",
      "3    2024-09-01    4.1\n",
      "4    2024-08-01    4.2\n",
      "..          ...    ...\n",
      "919  1948-05-01    3.5\n",
      "920  1948-04-01    3.9\n",
      "921  1948-03-01    4.0\n",
      "922  1948-02-01    3.8\n",
      "923  1948-01-01    3.4\n",
      "\n",
      "[924 rows x 2 columns]\n",
      "   fiscalDateEnding reportedEPS\n",
      "0        2024-12-31        6.41\n",
      "1        2023-12-31        9.61\n",
      "2        2022-12-31        9.12\n",
      "3        2021-12-31        9.97\n",
      "4        2020-12-31        8.67\n",
      "5        2019-12-31       12.81\n",
      "6        2018-12-31       13.82\n",
      "7        2017-12-31       13.65\n",
      "8        2016-12-31        13.6\n",
      "9        2015-12-31       14.93\n",
      "10       2014-12-31       16.27\n",
      "11       2013-12-31       17.03\n",
      "12       2012-12-31        15.3\n",
      "13       2011-12-31       13.49\n",
      "14       2010-12-31       11.58\n",
      "15       2009-12-31       10.01\n",
      "16       2008-12-31        8.96\n",
      "17       2007-12-31        7.19\n",
      "18       2006-12-31        6.09\n",
      "19       2005-12-31        5.34\n",
      "20       2004-12-31        5.07\n",
      "21       2003-12-31        4.34\n",
      "22       2002-12-31         3.9\n",
      "23       2001-12-31        4.44\n",
      "24       2000-12-31        4.45\n",
      "25       1999-12-31        3.71\n",
      "26       1998-12-31        3.32\n",
      "27       1997-12-31        3.09\n",
      "28       1996-12-31        2.77\n",
      "    fiscalDateEnding reportedDate reportedEPS estimatedEPS surprise  \\\n",
      "0         2024-09-30   2024-10-23         2.3         2.23     0.07   \n",
      "1         2024-06-30   2024-07-24        2.43          2.2     0.23   \n",
      "2         2024-03-31   2024-04-24        1.68          1.6     0.08   \n",
      "3         2023-12-31   2024-01-24        3.87         3.78     0.09   \n",
      "4         2023-09-30   2023-10-25         2.2         2.13     0.07   \n",
      "..               ...          ...         ...          ...      ...   \n",
      "110       1997-03-31   1997-04-23        0.59         0.58     0.01   \n",
      "111       1996-12-31   1997-01-21        0.98         0.99    -0.01   \n",
      "112       1996-09-30   1996-10-21        0.61         0.61        0   \n",
      "113       1996-06-30   1996-07-25        0.63         0.61     0.02   \n",
      "114       1996-03-31   1996-04-17        0.55          0.6    -0.05   \n",
      "\n",
      "    surprisePercentage   reportTime  \n",
      "0                3.139  post-market  \n",
      "1              10.4545  post-market  \n",
      "2                    5  post-market  \n",
      "3                2.381  post-market  \n",
      "4               3.2864  post-market  \n",
      "..                 ...          ...  \n",
      "110             1.7241   pre-market  \n",
      "111            -1.0101   pre-market  \n",
      "112                  0   pre-market  \n",
      "113             3.2787   pre-market  \n",
      "114            -8.3333   pre-market  \n",
      "\n",
      "[115 rows x 7 columns]\n",
      "      timestamp             value\n",
      "0    2024-11-01  166.240565127144\n",
      "1    2024-10-01  166.706583314313\n",
      "2    2024-09-01  161.463540590027\n",
      "3    2024-08-01  164.174770420668\n",
      "4    2024-07-01  166.137493006254\n",
      "..          ...               ...\n",
      "390  1992-05-01                 .\n",
      "391  1992-04-01                 .\n",
      "392  1992-03-01                 .\n",
      "393  1992-02-01                 .\n",
      "394  1992-01-01                 .\n",
      "\n",
      "[395 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=FEDERAL_FUNDS_RATE&interval=daily&apikey={apikey}&datatype=csv'\n",
    "df_fed = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=CPI&interval=monthly&apikey={apikey}&datatype=csv'\n",
    "df_inflation = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=UNEMPLOYMENT&apikey={apikey}&datatype=csv'\n",
    "df_unemployment = pd.read_csv(url)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={symbol}&apikey={apikey}'\n",
    "r = requests.get(url)\n",
    "dataAnnual = r.json()['annualEarnings']\n",
    "dataQuarter = r.json()['quarterlyEarnings']\n",
    "df_annual_earnings = pd.DataFrame(dataAnnual)\n",
    "df_quarterly_earnings = pd.DataFrame(dataQuarter)\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=ALL_COMMODITIES&interval=monthly&apikey={apikey}&datatype=csv'\n",
    "df_commodities = pd.read_csv(url)\n",
    "\n",
    "print(df_fed)\n",
    "print(df_inflation)\n",
    "print(df_unemployment)\n",
    "print(df_annual_earnings)\n",
    "print(df_quarterly_earnings)\n",
    "print(df_commodities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 title  \\\n",
      "0     Palantir Technologies Stock: Buy, Sell, or Hold?   \n",
      "1    IBM Partners With e& to Launch Multi-Model AI ...   \n",
      "2    IBM Stock Before Q4 Earnings: A Smart Buy or R...   \n",
      "3    Apple To $280? Here Are 10 Top Analyst Forecas...   \n",
      "4         This AI Stock Is Also a Great Dividend Stock   \n",
      "..                                                 ...   \n",
      "659  Mobile Value-added Services (VAS) Market to gr...   \n",
      "660  Electronic Shift Operations Management Solutio...   \n",
      "661  Data-as-a-Service (DaaS) Market size to grow b...   \n",
      "662  Cyber Weapon Market to grow by USD 8.08 billio...   \n",
      "663  Cyber Security Market size to grow by USD 203....   \n",
      "\n",
      "                                                   url   time_published  \\\n",
      "0    https://www.fool.com/investing/2025/01/24/pala...  20250124T110000   \n",
      "1    https://www.zacks.com/stock/news/2402332/ibm-p...  20250123T161100   \n",
      "2    https://www.zacks.com/stock/news/2402077/ibm-s...  20250123T142600   \n",
      "3    https://www.benzinga.com/25/01/43157122/apple-...  20250123T124951   \n",
      "4    https://www.fool.com/investing/2025/01/22/this...  20250122T113000   \n",
      "..                                                 ...              ...   \n",
      "659  https://www.prnewswire.com/news-releases/mobil...  20231124T220500   \n",
      "660  https://www.prnewswire.com/news-releases/elect...  20231124T193000   \n",
      "661  https://www.prnewswire.com/news-releases/data-...  20231124T031500   \n",
      "662  https://www.prnewswire.com/news-releases/cyber...  20231123T081500   \n",
      "663  https://www.prnewswire.com/news-releases/cyber...  20231123T041500   \n",
      "\n",
      "                         authors  \\\n",
      "0                   [Dan Victor]   \n",
      "1    [Zacks Investment Research]   \n",
      "2                 [Supriyo Bose]   \n",
      "3                   [Avi Kapoor]   \n",
      "4                [Timothy Green]   \n",
      "..                           ...   \n",
      "659                           []   \n",
      "660                           []   \n",
      "661                           []   \n",
      "662                           []   \n",
      "663                           []   \n",
      "\n",
      "                                               summary  \\\n",
      "0    Palantir Technologies ( NASDAQ: PLTR ) cemente...   \n",
      "1    IBM collaborates with e& to launch a cutting-e...   \n",
      "2    With declining earnings estimates, IBM is witn...   \n",
      "3    Top Wall Street analysts changed their outlook...   \n",
      "4    When you think about cutting-edge artificial i...   \n",
      "..                                                 ...   \n",
      "659  Mobile Value-added Services ( VAS ) Market to ...   \n",
      "660  Electronic Shift Operations Management Solutio...   \n",
      "661  Data-as-a-Service ( DaaS ) Market size to grow...   \n",
      "662  Cyber Weapon Market to grow by USD 8.08 billio...   \n",
      "663  Cyber Security Market size to grow by USD 203....   \n",
      "\n",
      "                                          banner_image            source  \\\n",
      "0    https://g.foolcdn.com/image/?url=https%3A%2F%2...       Motley Fool   \n",
      "1    https://staticx-tuner.zacks.com/images/article...  Zacks Commentary   \n",
      "2    https://staticx-tuner.zacks.com/images/article...  Zacks Commentary   \n",
      "3    https://cdn.benzinga.com/files/images/story/20...          Benzinga   \n",
      "4    https://g.foolcdn.com/editorial/images/804716/...       Motley Fool   \n",
      "..                                                 ...               ...   \n",
      "659                                               None       PR Newswire   \n",
      "660                                               None       PR Newswire   \n",
      "661                                               None       PR Newswire   \n",
      "662                                               None       PR Newswire   \n",
      "663                                               None       PR Newswire   \n",
      "\n",
      "    category_within_source       source_domain  \\\n",
      "0                      n/a        www.fool.com   \n",
      "1                      n/a       www.zacks.com   \n",
      "2                      n/a       www.zacks.com   \n",
      "3                  Trading    www.benzinga.com   \n",
      "4                      n/a        www.fool.com   \n",
      "..                     ...                 ...   \n",
      "659                    n/a  www.prnewswire.com   \n",
      "660                    n/a  www.prnewswire.com   \n",
      "661                    n/a  www.prnewswire.com   \n",
      "662                    n/a  www.prnewswire.com   \n",
      "663                    n/a  www.prnewswire.com   \n",
      "\n",
      "                                                topics  \\\n",
      "0    [{'topic': 'Earnings', 'relevance_score': '0.9...   \n",
      "1    [{'topic': 'Manufacturing', 'relevance_score':...   \n",
      "2    [{'topic': 'Financial Markets', 'relevance_sco...   \n",
      "3    [{'topic': 'Life Sciences', 'relevance_score':...   \n",
      "4    [{'topic': 'Technology', 'relevance_score': '1...   \n",
      "..                                                 ...   \n",
      "659  [{'topic': 'Technology', 'relevance_score': '1...   \n",
      "660  [{'topic': 'Technology', 'relevance_score': '0...   \n",
      "661  [{'topic': 'Technology', 'relevance_score': '0...   \n",
      "662  [{'topic': 'Technology', 'relevance_score': '0...   \n",
      "663  [{'topic': 'Retail & Wholesale', 'relevance_sc...   \n",
      "\n",
      "     overall_sentiment_score overall_sentiment_label  \\\n",
      "0                   0.382449                 Bullish   \n",
      "1                   0.332855        Somewhat-Bullish   \n",
      "2                   0.254890        Somewhat-Bullish   \n",
      "3                   0.244496        Somewhat-Bullish   \n",
      "4                   0.223838        Somewhat-Bullish   \n",
      "..                       ...                     ...   \n",
      "659                 0.229583        Somewhat-Bullish   \n",
      "660                 0.307821        Somewhat-Bullish   \n",
      "661                 0.295652        Somewhat-Bullish   \n",
      "662                 0.136210                 Neutral   \n",
      "663                 0.261836        Somewhat-Bullish   \n",
      "\n",
      "                                      ticker_sentiment  \n",
      "0    [{'ticker': 'MSFT', 'relevance_score': '0.0589...  \n",
      "1    [{'ticker': 'IDCC', 'relevance_score': '0.1478...  \n",
      "2    [{'ticker': 'MSFT', 'relevance_score': '0.0804...  \n",
      "3    [{'ticker': 'META', 'relevance_score': '0.1549...  \n",
      "4    [{'ticker': 'IBM', 'relevance_score': '0.45149...  \n",
      "..                                                 ...  \n",
      "659  [{'ticker': 'SSNLF', 'relevance_score': '0.074...  \n",
      "660  [{'ticker': 'VMW', 'relevance_score': '0.07587...  \n",
      "661  [{'ticker': 'LNVGF', 'relevance_score': '0.057...  \n",
      "662  [{'ticker': 'AVGO', 'relevance_score': '0.0647...  \n",
      "663  [{'ticker': 'AVGO', 'relevance_score': '0.0489...  \n",
      "\n",
      "[664 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={symbol}&apikey={apikey}&sort=LATEST&limit=1000'\n",
    "r = requests.get(url)\n",
    "data = r.json()['feed']\n",
    "df_sentiments = pd.DataFrame(data)\n",
    "print(df_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Lag 1'] = df['Close'].shift(1)\n",
    "df['7 Day Avg'] = df['Close'].rolling(window=7).mean()\n",
    "df['Daily Returns'] = (df['Close'] - df['Open']) / df['Open']\n",
    "df['Price to Volume Ratio'] = df['Close'] / df['Volume']\n",
    "df['Day of the Week'] = df['Date'].dt.dayofweek\n",
    "df['Quarter'] = df['Date'].dt.quarter\n",
    "df['Daily Return'] = df['Close'].pct_change()\n",
    "df['Volatility'] = df['High'] - df['Low']\n",
    "df['Price Volume Interaction'] = df['Daily Return'] * df['Volume']\n",
    "df['RSI * Volume'] = df['RSI'] * df['Volume']\n",
    "df['MACD / Bollinger Band Width'] = df['MACD'] / df['BB_Width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finlight key = sk_4470114c6d50ee45e7309af37287f659d87a2b8255f771e2d6815c36864bce5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0   Palantir Technologies Stock: Buy, Sell, or Hold?   \n",
      "1  IBM Partners With e& to Launch Multi-Model AI ...   \n",
      "2  IBM Stock Before Q4 Earnings: A Smart Buy or R...   \n",
      "3  Apple To $280? Here Are 10 Top Analyst Forecas...   \n",
      "4       This AI Stock Is Also a Great Dividend Stock   \n",
      "\n",
      "                                             summary  overall_sentiment_score  \\\n",
      "0  Palantir Technologies ( NASDAQ: PLTR ) cemente...                 0.382449   \n",
      "1  IBM collaborates with e& to launch a cutting-e...                 0.332855   \n",
      "2  With declining earnings estimates, IBM is witn...                 0.254890   \n",
      "3  Top Wall Street analysts changed their outlook...                 0.244496   \n",
      "4  When you think about cutting-edge artificial i...                 0.223838   \n",
      "\n",
      "  overall_sentiment_label                                   ticker_sentiment  \\\n",
      "0                 Bullish  [{'ticker': 'MSFT', 'relevance_score': '0.0589...   \n",
      "1        Somewhat-Bullish  [{'ticker': 'IDCC', 'relevance_score': '0.1478...   \n",
      "2        Somewhat-Bullish  [{'ticker': 'MSFT', 'relevance_score': '0.0804...   \n",
      "3        Somewhat-Bullish  [{'ticker': 'META', 'relevance_score': '0.1549...   \n",
      "4        Somewhat-Bullish  [{'ticker': 'IBM', 'relevance_score': '0.45149...   \n",
      "\n",
      "         Date  \n",
      "0  2025-01-24  \n",
      "1  2025-01-23  \n",
      "2  2025-01-23  \n",
      "3  2025-01-23  \n",
      "4  2025-01-22  \n"
     ]
    }
   ],
   "source": [
    "dropColumns = [1,3,5,6,7,8,9]\n",
    "\n",
    "df_sentiments.drop(df_sentiments.columns[dropColumns], axis = 1, inplace=True)\n",
    "\n",
    "df_sentiments['date'] = pd.to_datetime(df_sentiments['time_published'], format = '%Y%m%dT%H%M%S')\n",
    "df_sentiments['Date'] = df_sentiments['date'].dt.date\n",
    "\n",
    "dropColumns = [1, 6]\n",
    "df_sentiments.drop(df_sentiments.columns[dropColumns], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "print(df_sentiments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        title   summary  overall_sentiment_score overall_sentiment_label  \\\n",
      "0         0.0    0.0375                 0.382449                     0.0   \n",
      "1         0.0       0.0                 0.332855                     0.0   \n",
      "2    0.214286      -0.3                 0.254890                     0.0   \n",
      "3         0.5  0.366667                 0.244496                     0.0   \n",
      "4         0.8      -0.2                 0.223838                     0.0   \n",
      "..        ...       ...                      ...                     ...   \n",
      "659       0.0       0.0                 0.229583                     0.0   \n",
      "660       0.0       0.0                 0.307821                     0.0   \n",
      "661       0.0       0.0                 0.295652                     0.0   \n",
      "662       0.0       0.0                 0.136210                     0.0   \n",
      "663       0.0       0.0                 0.261836                     0.0   \n",
      "\n",
      "                                      ticker_sentiment        Date  \n",
      "0    [{'ticker': 'MSFT', 'relevance_score': '0.0589...  2025-01-24  \n",
      "1    [{'ticker': 'IDCC', 'relevance_score': '0.1478...  2025-01-23  \n",
      "2    [{'ticker': 'MSFT', 'relevance_score': '0.0804...  2025-01-23  \n",
      "3    [{'ticker': 'META', 'relevance_score': '0.1549...  2025-01-23  \n",
      "4    [{'ticker': 'IBM', 'relevance_score': '0.45149...  2025-01-22  \n",
      "..                                                 ...         ...  \n",
      "659  [{'ticker': 'SSNLF', 'relevance_score': '0.074...  2023-11-24  \n",
      "660  [{'ticker': 'VMW', 'relevance_score': '0.07587...  2023-11-24  \n",
      "661  [{'ticker': 'LNVGF', 'relevance_score': '0.057...  2023-11-24  \n",
      "662  [{'ticker': 'AVGO', 'relevance_score': '0.0647...  2023-11-23  \n",
      "663  [{'ticker': 'AVGO', 'relevance_score': '0.0489...  2023-11-23  \n",
      "\n",
      "[664 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "rows = len(df_sentiments.iloc[:,0])\n",
    "columns = ['title', 'summary', 'overall_sentiment_label']\n",
    "df_TB_sentiments = df_sentiments.copy()\n",
    "for column in columns:\n",
    "    for row in range(0, rows):\n",
    "        df_TB_sentiments.loc[row, column] = TextBlob(df_TB_sentiments.loc[row, column]).sentiment.polarity\n",
    "\n",
    "print(df_TB_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    title  summary  overall_sentiment_score  overall_sentiment_label  \\\n",
      "0  0.0000   0.8834                 0.382449                      0.0   \n",
      "1  0.5267   0.3182                 0.332855                      0.0   \n",
      "2  0.2263  -0.7096                 0.254890                      0.0   \n",
      "3  0.2023   0.5994                 0.244496                      0.0   \n",
      "4  0.6249   0.7506                 0.223838                      0.0   \n",
      "\n",
      "                                    ticker_sentiment        Date  \n",
      "0  [{'ticker': 'MSFT', 'relevance_score': '0.0589...  2025-01-24  \n",
      "1  [{'ticker': 'IDCC', 'relevance_score': '0.1478...  2025-01-23  \n",
      "2  [{'ticker': 'MSFT', 'relevance_score': '0.0804...  2025-01-23  \n",
      "3  [{'ticker': 'META', 'relevance_score': '0.1549...  2025-01-23  \n",
      "4  [{'ticker': 'IBM', 'relevance_score': '0.45149...  2025-01-22  \n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "df_vader_sentiments = df_sentiments.copy()\n",
    "columns = ['title', 'summary', 'overall_sentiment_label']\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "for column in columns:\n",
    "    df_vader_sentiments[column] = df_vader_sentiments[column].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "print(df_vader_sentiments.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TB_sentiments.drop(df_TB_sentiments.columns[[3, 4]], axis = 1, inplace = True)\n",
    "df_vader_sentiments.drop(df_vader_sentiments.columns[[3, 4]], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('ticker_sentiments.json', 'w') as file:\n",
    "    json.dump(df_sentiments['ticker_sentiment'][0], file, indent = 4)\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker  relevance_score  ticker_sentiment_score ticker_sentiment_label\n",
      "0   MSFT         0.058903                0.022053                Neutral\n",
      "1    IBM         0.058903                0.022053                Neutral\n",
      "2   PLTR         0.117486                0.341037       Somewhat-Bullish\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filePath = r\"C:\\Users\\ty725\\OneDrive\\Documents\\Winter Project\\ticker_sentiments.json\"\n",
    "df_ticker_sentiments = pd.read_json(filePath)\n",
    "print(df_ticker_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I figured this out kind of, just not sure how to incorporate this into my data. I will look into this in the future but for now we'll leave it. Will also figure out finlight api later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timestamp    open    high       low   close   volume\n",
      "6345  1999-11-01   42.63   42.97   42.4100   42.44   230800\n",
      "6344  1999-11-02   42.44   42.94   42.1400   42.25   156300\n",
      "6343  1999-11-03   42.88   43.13   42.6300   42.88   209200\n",
      "6342  1999-11-04   43.44   43.47   42.9100   43.20   409100\n",
      "6341  1999-11-05   44.25   44.25   43.4700   43.67   898000\n",
      "...          ...     ...     ...       ...     ...      ...\n",
      "4     2025-01-16  234.18  234.33  230.4500  230.50  2684261\n",
      "3     2025-01-17  234.66  234.94  233.0800  234.11  3371692\n",
      "2     2025-01-21  234.89  236.64  233.2601  236.06  3372076\n",
      "1     2025-01-22  239.25  241.95  238.9100  241.39  5121877\n",
      "0     2025-01-23  239.53  241.87  239.3000  241.82  3802235\n",
      "\n",
      "[6346 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "SectorTicker = {'COMMUNICATION SERVICES': 'XLC',\n",
    "              'CONSUMER DISCRETIONARY': 'XLY',\n",
    "              'CONSUMER STAPLES': 'XLP',\n",
    "              'ENERGY': 'XLE',\n",
    "              'FINANCIALS': 'XLF',\n",
    "              'HEALTHCARE': 'XLV',\n",
    "              'INDUSTRIALS': 'XLI',\n",
    "              'MATERIALS': 'XLB',\n",
    "              'REAL ESTATE': 'XLRE',\n",
    "              'TECHNOLOGY': 'XLK',\n",
    "              'UTILITIES': 'XLU'}\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={apikey}'\n",
    "r = requests.get(url)\n",
    "sectorName = r.json()['Sector']\n",
    "sectorSymbol = SectorTicker[sectorName]\n",
    "\n",
    "url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={sectorSymbol}&apikey={apikey}&datatype=csv&outputsize=full'\n",
    "df_sector = pd.read_csv(url)\n",
    "df_sector.sort_values(by = 'timestamp', inplace = True)\n",
    "print(df_sector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Data Cleaning\n",
    "Just to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date                           datetime64[ns]\n",
      "Open                                  float32\n",
      "High                                  float32\n",
      "Low                                   float32\n",
      "Close                                 float32\n",
      "Volume                                  int32\n",
      "SMA_10                                float64\n",
      "SMA_50                                float64\n",
      "SMA_100                               float64\n",
      "SMA_200                               float64\n",
      "EMA_10                                float64\n",
      "EMA_50                                float64\n",
      "EMA_100                               float64\n",
      "EMA_200                               float64\n",
      "RSI                                   float64\n",
      "MACD                                  float64\n",
      "MACD_signal                           float64\n",
      "Stochastic                            float64\n",
      "Bollinger_High                        float64\n",
      "Bollinger_Low                         float64\n",
      "ATR                                   float64\n",
      "OBV                                     int64\n",
      "Chaikin                               float64\n",
      "ADX                                   float64\n",
      "SAR_Down                              float64\n",
      "SAR_Up                                float64\n",
      "MA_Crossover                          float64\n",
      "BB_Width                              float64\n",
      "Lag 1                                 float32\n",
      "7 Day Avg                             float64\n",
      "Daily Returns                         float32\n",
      "Price to Volume Ratio                 float64\n",
      "Day of the Week                         int32\n",
      "Quarter                                 int32\n",
      "Daily Return                          float32\n",
      "Volatility                            float32\n",
      "Price Volume Interaction              float64\n",
      "RSI * Volume                          float64\n",
      "MACD / Bollinger Band Width           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(df.columns)\n",
    "for column in columns:\n",
    "    df[column] = df[column].bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiscalDateEnding    datetime64[ns]\n",
      "reportedEPS                float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_annual_earnings.columns[1] \n",
    "df_annual_earnings[column] = pd.to_numeric(df_annual_earnings[column], downcast='float')\n",
    "\n",
    "column = df_annual_earnings.columns[0]\n",
    "df_annual_earnings[column] = pd.to_datetime(df_annual_earnings[column])\n",
    "print(df_annual_earnings.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_commodities.columns[0]\n",
    "df_commodities[column] = pd.to_datetime(df_commodities[column])\n",
    "\n",
    "column = df_commodities.columns[1] \n",
    "df_commodities[column] = pd.to_numeric(df_commodities[column], errors='coerce', downcast='float')\n",
    "print(df_commodities.dtypes)\n",
    "df_commodities.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_dividend_date    datetime64[ns]\n",
      "amount                     float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_div.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_fed.columns[0]\n",
    "df_fed[column] = pd.to_datetime(df_fed[column])\n",
    "print(df_fed.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_inflation.columns[0]\n",
    "df_inflation[column] = pd.to_datetime(df_inflation[column])\n",
    "print(df_inflation.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_quarterly_earnings.drop(columns='fiscalDateEnding', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reportedDate          datetime64[ns]\n",
      "reportedEPS                  float32\n",
      "estimatedEPS                 float32\n",
      "surprise                     float32\n",
      "surprisePercentage           float32\n",
      "reportTime                      int8\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_quarterly_earnings.columns[0]\n",
    "df_quarterly_earnings[column] = pd.to_datetime(df_quarterly_earnings[column])\n",
    "\n",
    "columns = list(df_quarterly_earnings.columns)\n",
    "\n",
    "for num in range(1, len(columns) - 1):\n",
    "    df_quarterly_earnings[columns[num]] = pd.to_numeric(df_quarterly_earnings[columns[num]], downcast='float')\n",
    "df_quarterly_earnings['reportTime'] = df_quarterly_earnings['reportTime'].apply(lambda x: 1 if x == 'post-market' else -1 if x == 'pre-market' else 0)\n",
    "df_quarterly_earnings['reportTime'] = pd.to_numeric(df_quarterly_earnings['reportTime'], downcast='integer')\n",
    "print(df_quarterly_earnings.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp     object\n",
      "open         float64\n",
      "high         float64\n",
      "low          float64\n",
      "close        float64\n",
      "volume         int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_sector.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effective_date    datetime64[ns]\n",
      "split_factor             float32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_split.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall_sentiment_score    float64\n",
      "Date                        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_TB_sentiments.drop(df_TB_sentiments.columns[[0, 1]], axis = 1, inplace=True)\n",
    "print(df_TB_sentiments.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = df_TB_sentiments.columns[1]\n",
    "df_TB_sentiments[column] = pd.to_datetime(df_TB_sentiments[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    datetime64[ns]\n",
      "value               float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "column = df_unemployment.columns[0]\n",
    "df_unemployment[column] = pd.to_datetime(df_unemployment[column])\n",
    "print(df_unemployment.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader_sentiments.drop(df_vader_sentiments.columns[0], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary                           float64\n",
      "overall_sentiment_score           float64\n",
      "Date                       datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_vader_sentiments['Date'] = pd.to_datetime(df_vader_sentiments['Date'])\n",
    "print(df_vader_sentiments.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont think the textblob/vader experiment worked. The data looks a bit off so we're just going to stick with the overall sentiment score directly from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   overall_sentiment_score        Date\n",
      "0                 0.382449  2025-01-24\n",
      "1                 0.332855  2025-01-23\n",
      "2                 0.254890  2025-01-23\n",
      "3                 0.244496  2025-01-23\n",
      "4                 0.223838  2025-01-22\n"
     ]
    }
   ],
   "source": [
    "df_overall_sentiments = df_sentiments.copy()\n",
    "columns = [0, 1, 3, 4]\n",
    "df_overall_sentiments.drop(df_overall_sentiments.columns[columns], axis = 1, inplace=True)\n",
    "print(df_overall_sentiments.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'Date'\n",
    "df_annual_earnings.rename(columns = {'fiscalDateEnding': date, 'reportedEPS': 'AnnualEarnings'}, errors='ignore', inplace = True)\n",
    "df_commodities.rename(columns={'timestamp': date, 'value': 'Commodities_Index'}, errors='ignore', inplace=True)\n",
    "df_div.rename(columns = {'ex_dividend_date': date, 'amount':'Dividend'}, errors='ignore', inplace = True)\n",
    "df_fed.rename(columns = {'timestamp': date, 'value':'Fed_Rate'}, errors='ignore', inplace=True)\n",
    "df_inflation.rename(columns = {'timestamp': date, 'value':'Inflation'}, errors='ignore', inplace=True)\n",
    "df_quarterly_earnings.rename(columns = {'reportedDate': date, 'reportedEPS': 'QuarterEarnings', 'estimatedEPS': 'EstimatedQuarterEarnings', 'surprise': 'QuarterSurprise', 'surprisePercentage': 'QuarterSurprisePercentage', 'reportTime': 'QuarterReportTime'}, errors='ignore', inplace = True)\n",
    "df_sector.rename(columns = {'timestamp': date, 'open': 'SectorOpen', 'high':'SectorHigh', 'low':'SectorLow', 'close': 'SectorClose', 'volume':'SectorVolume'}, errors='ignore', inplace = True)\n",
    "df_split.rename(columns = {'effective_date': date}, errors='ignore', inplace = True)\n",
    "df_unemployment.rename(columns = {'timestamp': date, 'value':'UnemploymentRate'}, errors='ignore', inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Date Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserting is a success!\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "server = 'dahomey.database.windows.net'\n",
    "database = 'Stock Data'\n",
    "username = 'ttshiamala'\n",
    "password = 'Bear8486!?'\n",
    "driver = '{ODBC Driver 18 for SQL Server}'\n",
    "\n",
    "# Connect to Azure SQL\n",
    "connection_string = f\"DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}\"\n",
    "conn = pyodbc.connect(connection_string)\n",
    "\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "df.to_sql('Stock', engine, if_exists = 'replace', index=False)\n",
    "\n",
    "print('Data inserting is a success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_df = [df_annual_earnings, df_commodities, df_div, df_fed, df_inflation, df_overall_sentiments, df_quarterly_earnings, df_sector, df_split, df_unemployment]\n",
    "list_tables = ['AnnualEarnings', 'Commodities', 'Dividends', 'FederalFundsRate', 'Inflation', 'Sentiments', 'QuarterlyEarnings', 'SectorPrices', 'Split', 'Unemployment']\n",
    "\n",
    "conn = pyodbc.connect(connection_string)\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "for i, dataFrame in enumerate(list_df):\n",
    "    dataFrame.to_sql(list_tables[i], engine, if_exists = 'replace', index=False)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Query and Loading\n",
    "This query joins all the data tables that was collected and loaded onto the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Query.sql\", \"r\") as file:\n",
    "    query = file.read()\n",
    "\n",
    "conn = pyodbc.connect(connection_string)\n",
    "engine = create_engine('mssql+pyodbc://', creator=lambda: conn)\n",
    "finalDF = pd.read_sql(sql=query, con=engine, parse_dates=['Date'])\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps is to make sure that the data makes sense. For example, with unemployment, not all values were added because of left join so I may have to revisit how I joined these tables. Need to account for days that the stock wasn't trading (commodities as well). Also have to double check sentiments. Need to also consider the case that there were multiple stories in one day (average). After that I have to clean the dataset once more and handle missing values and things of that nature. Actually now that I'm taking a closer look at it, there are some spurious tables. It may be because of the overall sentiments for example with multiple dates.\n",
    "\n",
    "I fixed it the dataset is almost perfect. Just need to clean it one more time then it is ready for machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
